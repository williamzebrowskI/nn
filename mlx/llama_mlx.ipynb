{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, dims: int, num_heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads # Number of heads in the multi-head attention\n",
    "\n",
    "        self.rope = nn.RoPE(dims // num_heads, traditional=True) # Relative positional encoding meaning that the keys and queries will be augmented with the relative positional encoding\n",
    "        self.query_proj = nn.Linear(dims, dims, bias=False) # Linear projection for the queries\n",
    "        self.key_proj = nn.Linear(dims, dims, bias=False) # Linear projection for the keys\n",
    "        self.value_proj = nn.Linear(dims, dims, bias=False) # Linear projection for the values\n",
    "        self.out_proj = nn.Linear(dims, dims, bias=False) # Linear projection for the output\n",
    "\n",
    "    def __call__(self, queries, keys, values, mask=None, cache=None):\n",
    "        queries = self.query_proj(queries) # Project the queries\n",
    "        keys = self.key_proj(keys)  # Project the keys\n",
    "        values = self.value_proj(values)    # Project the values\n",
    "\n",
    "        # Extract some shapes\n",
    "        num_heads = self.num_heads # Number of heads\n",
    "        B, L, D = queries.shape # Batch size, sequence length and dimensionality for the queries\n",
    "\n",
    "        # Prepare the queries, keys and values for the attention computation\n",
    "        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3) # Reshape the queries to have the shape (B, num_heads, L, D // num_heads) for the attention computation\n",
    "        print(queries)\n",
    "        keys = keys.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3) # Reshape the keys to have the shape (B, num_heads, L, D // num_heads) for the attention computation\n",
    "        values = values.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3) # Reshape the values to have the shape (B, num_heads, L, D // num_heads) for the attention computation\n",
    "\n",
    "        # Add RoPE to the queries and keys and combine them with the cache\n",
    "        if cache is not None:\n",
    "            key_cache, value_cache = cache # Unpack the cache\n",
    "            queries = self.rope(queries, offset=key_cache.shape[2]) # Add RoPE to the queries\n",
    "            keys = self.rope(keys, offset=key_cache.shape[2]) # Add RoPE to the keys\n",
    "            keys = mx.concatenate([key_cache, keys], axis=2) # Concatenate the keys with the cache\n",
    "            values = mx.concatenate([value_cache, values], axis=2) # Concatenate the values with the cache\n",
    "        else: # If there is no cache\n",
    "            queries = self.rope(queries) # Add RoPE to the queries\n",
    "            keys = self.rope(keys) # Add RoPE to the keys\n",
    "\n",
    "        # Finally perform the attention computation\n",
    "        scale = math.sqrt(1 / queries.shape[-1]) # Compute the scale for the attention computation\n",
    "        scores = (queries * scale) @ keys.transpose(0, 1, 3, 2) # Compute the attention scores\n",
    "        if mask is not None: # If there is a mask\n",
    "            scores = scores + mask # Add the mask to the scores\n",
    "        scores = mx.softmax(scores, axis=-1) # Compute the attention weights\n",
    "        values_hat = (scores @ values).transpose(0, 2, 1, 3).reshape(B, L, -1) # Compute the output values\n",
    "\n",
    "        # Note that we return the keys and values to possibly be used as a cache\n",
    "        return self.out_proj(values_hat), (keys, values) # Return the output values and the keys and values to be used as a cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
