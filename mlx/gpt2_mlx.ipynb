{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce GPT2 with MLX\n",
    "\n",
    "This document provides a detailed breakdown of the implementation and training process for a transformer-based model, including the GPT model architecture, multi-head attention mechanism, multi-layer perceptron (MLP), and individual transformer blocks in Apple's MLX. The explanations are designed to help you understand each component's role and functionality within the broader model structure. By the end of this document, you should have a comprehensive understanding of how to implement and train a transformer model and code on in MLX, as well as how each piece fits together to form a powerful machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [GPT Model Implementation](#gpt-model-implementation)\n",
    "  - [GPT Model Class Definition](#gpt-model-class-definition)\n",
    "  - [Initialization Method (`__init__`)](#initialization-method-__init__)\n",
    "  - [Generate Method (`generate`)](#generate-method-generate)\n",
    "  - [Parameter Initialization Method (`_init_parameters`)](#parameter-initialization-method-_init_parameters)\n",
    "  - [Forward Pass Method (`__call__`)](#forward-pass-method-__call__)\n",
    "\n",
    "- [Multi-Head Attention Implementation](#multi-head-attention-implementation)\n",
    "  - [Multi-Head Attention Class Definition](#multi-head-attention-class-definition)\n",
    "  - [Initialization Method (`__init__`)](#initialization-method-__init__)\n",
    "  - [Forward Pass Method (`__call__`)](#forward-pass-method-__call__)\n",
    "\n",
    "- [Multi-Layer Perceptron (MLP) Implementation](#multi-layer-perceptron-mlp-implementation)\n",
    "  - [MLP Class Definition](#mlp-class-definition)\n",
    "  - [Attributes](#attributes)\n",
    "  - [Initialization Method (`__init__`)](#initialization-method-__init__)\n",
    "  - [Forward Pass Method (`__call__`)](#forward-pass-method-__call__)\n",
    "\n",
    "- [Transformer Block Implementation](#transformer-block-implementation)\n",
    "  - [Block Class Definition](#block-class-definition)\n",
    "  - [Initialization Method (`__init__`)](#initialization-method-__init__)\n",
    "  - [Forward Pass Method (`__call__`)](#forward-pass-method-__call__)\n",
    "\n",
    "- [Training Loop Implementation](#training-loop-implementation)\n",
    "  - [Training Loop Overview](#training-loop-overview)\n",
    "  - [Training Phase](#training-phase)\n",
    "  - [Evaluation Phase](#evaluation-phase)\n",
    "  - [Logging](#logging)\n",
    "  - [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mlx numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Install mlx and run the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import mlx.utils as utils\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to training an LLM is collecting a large corpus of text data and then tokenizing it. Tokenization is the process of mapping text to integers, which can be fed into the LLM. Our training corpus for this model will be the works of Shakespeare concatenated into one file. This is roughly 1 million characters and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "with open('../input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the file as a single long string into the text variable. Then we use the set() function to get all the unique characters in the text which will be our vocabulary. By printing vocab you can see all the characters in our vocabulary as one string, and we have a total of 65 characters which till be our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(text)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then wrap a list around it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G',\n",
       " '\\n',\n",
       " 'B',\n",
       " 'v',\n",
       " 'L',\n",
       " \"'\",\n",
       " ';',\n",
       " '!',\n",
       " 't',\n",
       " 'a',\n",
       " ',',\n",
       " 'Z',\n",
       " 'C',\n",
       " ':',\n",
       " 'W',\n",
       " 'R',\n",
       " '$',\n",
       " 'F',\n",
       " 'x',\n",
       " 'I',\n",
       " 'i',\n",
       " ' ',\n",
       " 'c',\n",
       " 'J',\n",
       " 'Y',\n",
       " 'Q',\n",
       " 'l',\n",
       " 'p',\n",
       " '?',\n",
       " 'O',\n",
       " '&',\n",
       " 'X',\n",
       " 'w',\n",
       " 'g',\n",
       " 'T',\n",
       " 'q',\n",
       " 'd',\n",
       " '-',\n",
       " 'K',\n",
       " 'y',\n",
       " '.',\n",
       " 'D',\n",
       " 'U',\n",
       " 'P',\n",
       " 'N',\n",
       " 'e',\n",
       " 'j',\n",
       " 'z',\n",
       " 'o',\n",
       " 'k',\n",
       " 'M',\n",
       " 'b',\n",
       " 'm',\n",
       " 'E',\n",
       " 'A',\n",
       " '3',\n",
       " 'h',\n",
       " 'r',\n",
       " 'n',\n",
       " 'S',\n",
       " 'H',\n",
       " 'V',\n",
       " 'u',\n",
       " 'f',\n",
       " 's']"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(text))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll sort it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"vocab_size: {vocab_size}\")\n",
    "\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from vocab to integers\n",
    "itos = {i:c for i,c in enumerate(vocab)} # int to string\n",
    "stoi = {c:i for i,c in enumerate(vocab)} # string to int\n",
    "encode = lambda x: [stoi[c] for c in x] # encode string to int\n",
    "decode = lambda x: ''.join([itos[i] for i in x]) # decode int to string\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "# [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
    "print(decode(encode(\"hello world\")))\n",
    "# hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encode(text) # encode the entire text\n",
    "split = int(0.9 * len(data)) # 90% train, 10% validation\n",
    "train_data = data[:split] # first 90%\n",
    "val_data = data[split:] # last 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58]\n"
     ]
    }
   ],
   "source": [
    "ctx_len = 4\n",
    "print(train_data[:ctx_len + 1])\n",
    "# [18, 47, 56, 57, 58,  1, 15, 47, 58]\n",
    "# x: [18, 47, 56, 57, 58,  1, 15, 47] | y: 58\n",
    "\n",
    "# 8 sub examples\n",
    "# [18] --> 47\n",
    "# [18, 47] --> 56\n",
    "# [18, 47, 56] --> 57\n",
    "# [18, 47, 56, 57] --> 58\n",
    "# [18, 47, 56, 57, 58] --> 1\n",
    "# [18, 47, 56, 57, 58, 1] --> 15\n",
    "# [18, 47, 56, 57, 58, 1, 15] --> 47\n",
    "# [18, 47, 56, 57, 58, 1, 15, 47] --> 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  [18, 47, 56, 57]\n",
      "labels:  [47, 56, 57, 58]\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs: \", train_data[:ctx_len])\n",
    "print(\"labels: \", train_data[1:ctx_len+1]) # labels = inputs indexed 1 higher\n",
    "# inputs: [18, 47, 56, 57, 58,  1, 15, 47]\n",
    "# labels: [47, 56, 57, 58,  1, 15, 47, 58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation datasets\n",
    "ctx_len = 4\n",
    "X_train = mx.array([train_data[i:i+ctx_len] for i in range(0, len(train_data) - ctx_len, ctx_len)]) \n",
    "y_train = mx.array([train_data[i+1:i+ctx_len+1] for i in range(0, len(train_data) - ctx_len, ctx_len)]) \n",
    "X_val = mx.array([val_data[i:i+ctx_len] for i in range(0, len(val_data) - ctx_len, ctx_len)])\n",
    "y_val = mx.array([val_data[i+1:i+ctx_len+1] for i in range(0, len(val_data) - ctx_len, ctx_len)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, y, b_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        ix = np.arange(X.shape[0])\n",
    "        np.random.shuffle(ix)\n",
    "        ix = mx.array(ix)\n",
    "        X = X[ix]\n",
    "        y = y[ix]\n",
    "    for i in range(0, X.shape[0], b_size):\n",
    "        input = X[i:i+b_size]\n",
    "        label = y[i:i+b_size]\n",
    "        yield input, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice the output below for the label array, 'y', is shifted to the left by 1 in order to predict the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[40, 43, 42, 57]], dtype=int32)\n",
      "array([[43, 42, 57, 10]], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "batch = get_batches(X_train, y_train, 1)\n",
    "for X, y in batch:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_len = 128\n",
    "n_emb = 128\n",
    "dropout = 0.1\n",
    "head_size = 128\n",
    "n_heads = 4 \n",
    "n_layers = 3 \n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, head_size):\n",
    "#         super().__init__()\n",
    "#         self.head_size = head_size # Define the head size of the attention mechanism \n",
    "#         self.k_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the key projection\n",
    "#         self.q_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the query projection\n",
    "#         self.v_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the value projection\n",
    "#         indices = mx.arange(ctx_len) # Create a tensor with values from 0 to ctx_len - 1\n",
    "#         print(f\"indices: \\n {indices} \\n\")\n",
    "#         mask = indices[:, None] < indices[None] # If the value of the first tensor is less than the value of the second tensor, the value of the mask tensor is True, otherwise False which means that the mask tensor is a lower triangular matrix\n",
    "#         print(f\"mask: \\n {mask} \\n\")\n",
    "#         self._causal_mask = mask * -1e9 # Multiply the mask tensor by -1e9 to get a tensor with -1e9 where the value of the first tensor is less than the value of the second tensor\n",
    "#         print(f\"mask: \\n {self._causal_mask} \\n\")\n",
    "#         self.c_proj = nn.Linear(head_size, n_emb) # output projection layer to get the output of the attention mechanism\n",
    "#         self.resid_dropout = nn.Dropout(dropout) # Define the dropout layer for the residual connection\n",
    "#     def __call__(self, x): # shapes commented\n",
    "#         B, T, C = x.shape # (batch_size, ctx_len, n_emb) - x is the input tensor\n",
    "#         K = self.k_proj(x) # (B, T, head_size) - Project the keys\n",
    "#         Q = self.q_proj(x) # (B, T, head_size) - Project the queries\n",
    "#         V = self.v_proj(x) # (B, T, head_size) - Project the values\n",
    "#         attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size) # We use K.transpose([0, 2, 1]) to transpose the second and third dimensions of K. This is because we want to multiply the queries with the keys. The shape of the attention weights is (B, T, T) \n",
    "#         # attn_weights.shape = (B, T, T)\n",
    "#         attn_weights = attn_weights + self._causal_mask # Add the causal mask to the attention weights\n",
    "#         attn_weights = mx.softmax(attn_weights, axis=-1) # Apply the softmax function to the attention weights to get the attention scores\n",
    "#         o = (attn_weights @ V) # (B, T, head_size) - Multiply the attention scores with the values to get the output\n",
    "#         o = self.c_proj(self.resid_dropout(o)) # (B, T, n_emb) - Apply the output projection layer to the output\n",
    "#         return o # Return the output of the attention mechanism which will be used as the input to the feedforward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Class\n",
    "\n",
    "The `MultiHeadAttention` class is a subclass of `nn.Module`, designed to perform the attention mechanism across multiple heads in parallel. This allows the model to focus on different parts of the input sequence simultaneously.\n",
    "\n",
    "### Initialization Method (`__init__`)\n",
    "\n",
    "The initialization method sets up the following components:\n",
    "\n",
    "- **Head Size (`self.head_size`)**: Defines the size of each attention head. The total dimension of the attention mechanism is divided among these heads.\n",
    "- **Key Projection (`self.k_proj`)**: A linear layer that projects the input embeddings into key vectors of the specified `head_size`.\n",
    "- **Query Projection (`self.q_proj`)**: A linear layer that projects the input embeddings into query vectors of the specified `head_size`.\n",
    "- **Value Projection (`self.v_proj`)**: A linear layer that projects the input embeddings into value vectors of the specified `head_size`.\n",
    "- **Causal Mask (`self._causal_mask`)**: A mask that ensures the model cannot attend to future tokens in the sequence. It is implemented as a lower triangular matrix with large negative values (`-1e9`) to effectively nullify certain attention weights.\n",
    "- **Output Projection (`self.c_proj`)**: A linear layer that projects the concatenated output of all heads back into the original embedding dimension (`n_emb`).\n",
    "- **Attention Dropout (`self.attn_dropout`)**: A dropout layer applied to the attention weights to prevent overfitting.\n",
    "- **Residual Dropout (`self.resid_dropout`)**: A dropout layer applied to the output of the attention mechanism before the final projection.\n",
    "\n",
    "### Forward Pass Method (`__call__`)\n",
    "\n",
    "The `__call__` method defines the forward pass of the multi-head attention mechanism:\n",
    "\n",
    "- **Input Shape**: The input tensor `x` has the shape `(B, T, C)` where `B` is the batch size, `T` is the context length (number of tokens), and `C` is the embedding dimension.\n",
    "- **Key, Query, Value Projections**: The input tensor is projected into key (`K`), query (`Q`), and value (`V`) tensors using the respective linear layers. Each projection has the shape `(B, T, head_size)`.\n",
    "- **Reshape for Multi-Head Attention**: The projected tensors are reshaped to enable multi-head attention. The new shape is `(B, n_heads, T, head_size//n_heads)`, allowing each head to focus on different parts of the sequence.\n",
    "- **Attention Weight Calculation**: The attention weights are computed by taking the dot product of the query and key matrices, followed by scaling by the square root of the key dimension. The shape of the attention weights is `(B, n_heads, T, T)`.\n",
    "- **Causal Masking**: The causal mask is added to the attention weights to prevent attending to future tokens.\n",
    "- **Softmax and Dropout**: The masked attention weights are passed through a softmax function to normalize them into probabilities. Dropout is then applied to these attention weights.\n",
    "- **Weighted Sum of Values**: The attention weights are used to compute a weighted sum of the value vectors, producing the attention output for each head.\n",
    "- **Output Reshape and Projection**: The multi-head attention output is reshaped back to the original embedding dimensions and passed through the output projection layer. Dropout is applied before the final projection.\n",
    "- **Final Output**: The method returns the output of the attention mechanism, which will be used as input to the subsequent layers, typically a feedforward neural network.\n",
    "\n",
    "This implementation of multi-head attention allows the model to attend to different parts of the input sequence simultaneously, enhancing its ability to capture complex dependencies within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size # Define the head size of the attention mechanism \n",
    "        self.k_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the key projection\n",
    "        self.q_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the query projection\n",
    "        self.v_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the value projection\n",
    "        indices = mx.arange(ctx_len) # Create a tensor with values from 0 to ctx_len - 1\n",
    "        # print(f\"indices: \\n {indices} \\n\")\n",
    "        mask = indices[:, None] < indices[None] # If the value of the first tensor is less than the value of the second tensor, the value of the mask tensor is True, otherwise False which means that the mask tensor is a lower triangular matrix\n",
    "        # print(f\"mask: \\n {mask} \\n\")\n",
    "        self._causal_mask = mask * -1e9 # Multiply the mask tensor by -1e9 to get a tensor with -1e9 where the value of the first tensor is less than the value of the second tensor\n",
    "        # print(f\"mask: \\n {self._causal_mask} \\n\")\n",
    "        self.c_proj = nn.Linear(head_size, n_emb) # output projection layer to get the output of the attention mechanism\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout) # Define the dropout layer for the residual connection\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def __call__(self, x): # shapes commented\n",
    "        B, T, C = x.shape # (batch_size, ctx_len, n_emb) - x is the input tensor\n",
    "        K = self.k_proj(x) # (B, T, head_size) - Project the keys\n",
    "        Q = self.q_proj(x) # (B, T, head_size) - Project the queries\n",
    "        V = self.v_proj(x) # (B, T, head_size) - Project the values\n",
    "        mha_shape = (B, T, n_heads, head_size//n_heads) # This is the shape of the multi-head attention mechanism because we want to split the head_size into n_heads\n",
    "        # print(f\"mha_shape: \\n {mha_shape} \\n\")\n",
    "        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # We use mx.as_strided to create a view of the K tensor with the shape (B, n_heads, T, head_size//n_heads) and then transpose the dimensions to get the desired shape\n",
    "        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # We use mx.as_strided to create a view of the Q tensor with the shape (B, n_heads, T, head_size//n_heads) and then transpose the dimensions to get the desired shape\n",
    "        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # We use mx.as_strided to create a view of the V tensor with the shape (B, n_heads, T, head_size//n_heads) and then transpose the dimensions to get the desired shape\n",
    "        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # We use K.transpose([0, 1, 3, 2]) to transpose the second and third dimensions of K. This is because we want to multiply the queries with the keys. The shape of the attention weights is (B, n_heads, T, T)\n",
    "        # print(f\"attn_weights: \\n {attn_weights} \\n\")\n",
    "        attn_weights = attn_weights + self._causal_mask[:T, :T] # Add the causal mask to the attention weights\n",
    "        # print(f\"attn_weights + casual mask: \\n {attn_weights} \\n\")\n",
    "        attn_weights = mx.softmax(attn_weights, axis=-1) # Apply the softmax function to the attention weights to get the attention scores\n",
    "        # print(f\"softmax attn_weights: \\n {attn_weights} \\n\")\n",
    "        attn_weights = self.attn_dropout(attn_weights) # Apply the dropout layer to the attention weights\n",
    "        # print(f\"dropout attn_weights: \\n {attn_weights} \\n\")\n",
    "        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads) - Multiply the attention scores with the values to get the output\n",
    "        # print(f\"output: \\n {o} \\n\")\n",
    "        o = o.transpose([0, 2, 1, 3]).reshape((B, T, head_size)) # We transpose the dimensions of the output and then reshape it to get the desired shape\n",
    "        # print(f\"output reshaped: \\n {o} \\n\")\n",
    "        o = self.c_proj(self.resid_dropout(o)) # (B, T, n_emb) - Apply the output projection layer to the output\n",
    "        # print(f\"output projection: \\n {o} \\n\")\n",
    "        return o # Return the output of the attention mechanism which will be used as the input to the feedforward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Class Definition\n",
    "\n",
    "The `MLP` class is a subclass of `nn.Module`, which defines a straightforward feedforward neural network used within a larger model, such as a transformer.\n",
    "\n",
    "### Attributes\n",
    "\n",
    "- **Fully Connected Layer (`c_fc`)**: This is a linear layer that expands the input dimension from `n_emb` to `4 * n_emb`. It performs a fully connected operation on the input tensor.\n",
    "- **GELU Activation (`gelu`)**: The GELU (Gaussian Error Linear Unit) activation function is applied after the fully connected layer to introduce non-linearity into the model.\n",
    "- **Projection Layer (`c_proj`)**: Another linear layer that projects the expanded dimension back down to the original embedding size (`n_emb`). This layer helps in controlling the model's capacity.\n",
    "- **Dropout Layer (`dropout`)**: A dropout layer is applied for regularization, helping to prevent overfitting by randomly setting some of the activations to zero during training.\n",
    "\n",
    "### Initialization Method (`__init__`)\n",
    "\n",
    "The initialization method sets up the layers of the MLP:\n",
    "\n",
    "- **c_fc**: This linear layer takes an input of size `n_emb` and expands it to `4 * n_emb`. This expansion allows the network to capture more complex patterns.\n",
    "- **gelu**: The GELU activation function is applied to the output of the `c_fc` layer, introducing non-linearity and enabling the model to learn more complex relationships.\n",
    "- **c_proj**: This linear layer reduces the dimension back to `n_emb`, balancing the expansion performed by `c_fc`.\n",
    "- **dropout**: Dropout is applied after the projection layer to regularize the network, reducing the risk of overfitting.\n",
    "\n",
    "### Forward Pass Method (`__call__`)\n",
    "\n",
    "The `__call__` method defines the forward pass of the MLP:\n",
    "\n",
    "- **Input to Fully Connected Layer**: The input tensor `x` is passed through the `c_fc` layer, which increases its dimensionality to `4 * n_emb`.\n",
    "- **GELU Activation**: The expanded tensor is then passed through the GELU activation function, introducing non-linearity.\n",
    "- **Projection Layer**: The activated tensor is passed through the `c_proj` layer, reducing its dimensionality back to `n_emb`.\n",
    "- **Dropout**: Dropout is applied to the output of the projection layer for regularization.\n",
    "- **Output**: The method returns the output tensor, which is now ready to be used in subsequent layers of the model.\n",
    "\n",
    "This implementation of the MLP is commonly used in transformer models, where it acts as the feedforward network within each transformer block, helping the model learn complex mappings between the input and output spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(n_emb, 4 * n_emb)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        x = self.gelu(self.c_fc(x))\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block Class Definition\n",
    "\n",
    "The `Block` class is a subclass of `nn.Module` that defines a single transformer block. This block combines a multi-head attention mechanism with a feedforward neural network (MLP) and includes layer normalization and residual connections.\n",
    "\n",
    "### Initialization Method (`__init__`)\n",
    "\n",
    "The initialization method sets up the following components:\n",
    "\n",
    "- **Multi-Layer Perceptron (`self.mlp`)**: An instance of the `MLP` class, which is a simple feedforward neural network used within the transformer block.\n",
    "- **Multi-Head Attention (`self.mha`)**: An instance of the `MultiHeadAttention` class, which allows the block to attend to different parts of the input sequence simultaneously.\n",
    "- **Layer Normalization 1 (`self.ln_1`)**: A layer normalization layer that normalizes the input tensor before it is passed to the multi-head attention mechanism.\n",
    "- **Layer Normalization 2 (`self.ln_2`)**: Another layer normalization layer that normalizes the input tensor before it is passed to the feedforward neural network (MLP).\n",
    "\n",
    "### Forward Pass Method (`__call__`)\n",
    "\n",
    "The `__call__` method defines the forward pass of the transformer block:\n",
    "\n",
    "- **Multi-Head Attention with Residual Connection**: \n",
    "  - The input tensor `x` is first normalized using `ln_1`.\n",
    "  - The normalized tensor is then passed through the multi-head attention mechanism (`mha`).\n",
    "  - The output of the attention mechanism is added to the original input tensor, creating a residual connection. This helps in preserving the original information while also incorporating the attention-modified information.\n",
    "\n",
    "- **Feedforward Neural Network (MLP) with Residual Connection**:\n",
    "  - The output from the previous step is normalized using `ln_2`.\n",
    "  - The normalized tensor is then passed through the MLP (`mlp`).\n",
    "  - The output of the MLP is added to the input tensor from the previous step, creating another residual connection. This further refines the information while maintaining the integrity of the original input.\n",
    "\n",
    "- **Final Output**: The method returns the final output tensor, which has been processed by both the multi-head attention mechanism and the feedforward neural network, with normalization and residual connections at each step.\n",
    "\n",
    "This transformer block is a core component of transformer models, where multiple such blocks are stacked to form a deep network capable of learning complex patterns and dependencies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = MLP() # Feedforward neural network\n",
    "        self.mha = MultiHeadAttention() # Multi-head attention mechanism\n",
    "        self.ln_1 = nn.LayerNorm(dims=n_emb) # Layer normalization layer\n",
    "        self.ln_2 = nn.LayerNorm(dims=n_emb) # Layer normalization layer\n",
    "    def __call__(self, x): \n",
    "        x = x + self.mha(self.ln_1(x)) # Add the output of the multi-head attention mechanism to the input tensor\n",
    "        x = x + self.mlp(self.ln_2(x)) # Add the output of the feedforward neural network to the input tensor \n",
    "        return x # Return the output of the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y):\n",
    "    logits = model(x)\n",
    "    B, T, C = logits.shape # (batch_size, seq_len, vocab_size)\n",
    "    logits = logits.reshape(B*T, C)\n",
    "    y = y.reshape(B*T)\n",
    "    loss = nn.losses.cross_entropy(logits, y, reduction='mean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Model Class Definition\n",
    "\n",
    "The `GPT` class is a subclass of `nn.Module`, which defines the architecture and operations of the GPT model. The model includes an embedding layer for both tokens and positions, a series of transformer blocks, and a final layer normalization followed by a linear projection for output.\n",
    "\n",
    "### Initialization Method (`__init__`)\n",
    "\n",
    "The initialization method sets up the following components:\n",
    "\n",
    "- **Word Embedding (`self.wte`)**: This is a lookup table that maps each token in the vocabulary to its corresponding embedding vector.\n",
    "- **Position Embedding (`self.wpe`)**: This embedding layer assigns a unique embedding vector to each position in the input sequence.\n",
    "- **Transformer Blocks (`self.blocks`)**: A sequence of transformer blocks that processes the input embeddings. The number of blocks is determined by `n_layers`.\n",
    "- **Layer Normalization (`self.ln_f`)**: A normalization layer applied to the output of the transformer blocks.\n",
    "- **Output Projection (`self.lm_head`)**: A linear layer that projects the final hidden state into the vocabulary space, producing logits for each token in the vocabulary.\n",
    "- **Parameter Initialization (`self._init_parameters`)**: This method initializes the parameters of the model, including weights and biases, using specific initialization strategies.\n",
    "\n",
    "### Generate Method (`generate`)\n",
    "\n",
    "The `generate` method is responsible for generating text by predicting the next token iteratively:\n",
    "\n",
    "- **Context Initialization**: A tensor of zeros is created to serve as the initial context.\n",
    "- **Token Generation Loop**: For each step in the generation process, the current context is passed through the model to generate logits, which are used to sample the next token.\n",
    "- **Context Update**: The sampled token is appended to the context, and the process repeats until the desired number of tokens is generated.\n",
    "\n",
    "### Parameter Initialization Method (`_init_parameters`)\n",
    "\n",
    "This method initializes the model's parameters with specific distributions:\n",
    "\n",
    "- **Normal Initialization (`normal_init`)**: Used for most parameters, with a mean of 0 and a standard deviation of 0.02.\n",
    "- **Residual Initialization (`residual_init`)**: Applied to residual layers, with a mean of 0 and a standard deviation adjusted by the number of layers.\n",
    "- **Bias Initialization**: Biases are initialized to zero.\n",
    "- **Updating Parameters**: The initialized parameters are applied to the model.\n",
    "\n",
    "### Forward Pass Method (`__call__`)\n",
    "\n",
    "The `__call__` method defines the forward pass of the GPT model:\n",
    "\n",
    "- **Token Embeddings**: The input tokens are converted to embeddings using the word embedding layer.\n",
    "- **Position Embeddings**: Positional information is added by summing token embeddings with position embeddings.\n",
    "- **Transformer Blocks**: The combined embeddings are passed through the series of transformer blocks.\n",
    "- **Layer Normalization**: The output of the transformer blocks is normalized.\n",
    "- **Output Logits**: The final output is projected into the vocabulary space to produce logits for each token in the vocabulary.\n",
    "\n",
    "This method returns the logits, which can be used for various tasks such as text generation, classification, or translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPT(nn.Module): # Define the GPT model\n",
    "    def __init__(self):\n",
    "        super().__init__() # Call the __init__ of the parent class\n",
    "        self.wte = nn.Embedding(vocab_size, n_emb) # Lookup table for embeddings of each token in the vocab (word to embedding) -  n_emb means the size of the embedding vector\n",
    "        self.wpe = nn.Embedding(ctx_len, n_emb) # Lookup table for embeddings of each position in the context (position to embedding)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block() for _ in range(n_layers)],\n",
    "        ) # transformer blocks - n_layers means the number of transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(dims=n_emb) # final layernorm\n",
    "        # print(f\"layernorm: \\n {self.ln_f} \\n\")\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size) # output projection\n",
    "        # print(f\"lm_head: \\n {self.lm_head} \\n\")\n",
    "        self._init_parameters() # Initialize the parameters of the model\n",
    "        # print total number of params on initialization\n",
    "        total_params = sum([p.size for n,p in utils.tree_flatten(self.parameters())]) # Get the total number of parameters\n",
    "        # print(f\"Total params: {(total_params / 1e6):.3f}M\") # Print the total number of parameters in millions\n",
    "\n",
    "    # method of GPT class\n",
    "    def generate(self, max_new_tokens):\n",
    "        ctx = mx.zeros((1, 1), dtype=mx.int32) # (1, 1) - Create a context tensor with zeros\n",
    "        for _ in range(max_new_tokens): # Loop through the number of tokens to generate\n",
    "            logits = self(ctx[:, -ctx_len:]) # pass in last ctx_len characters to get the next token\n",
    "            logits = logits[:, -1, :] # get logits for the next token only\n",
    "            next_tok = mx.random.categorical(logits, num_samples=1) # sample the next token\n",
    "            ctx = mx.concatenate((ctx, next_tok), axis=1) # append the next token to the context\n",
    "        return ctx # return the context\n",
    "    \n",
    "    # method of GPT\n",
    "    def _init_parameters(self):\n",
    "        normal_init = nn.init.normal(mean=0.0, std=0.02) # Initialize the weights with a normal distribution\n",
    "        residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers))) # Initialize the residuals with a normal distribution\n",
    "        new_params = [] # Create a list to store the new parameters\n",
    "        # print(f\"named_modules: \\n {self.named_modules()} \\n\")\n",
    "        for name, module in self.named_modules(): # Loop through the modules of the model\n",
    "            if isinstance(module, nn.layers.linear.Linear): # Check if the module is a linear layer\n",
    "                if 'c_proj' in name: # residual projection layer\n",
    "                    new_params.append((name + '.weight', residual_init(module.weight))) # Initialize the weights of the residual projection layer\n",
    "                else:\n",
    "                    new_params.append((name + '.weight', normal_init(module.weight))) # Initialize the weights of the linear layer\n",
    "                if 'bias' in module: # Check if the module has a bias\n",
    "                    new_params.append((name + '.bias', mx.zeros(module.bias.shape))) # Initialize the bias with zeros\n",
    "            elif isinstance(module, nn.layers.embedding.Embedding): # Check if the module is an embedding layer\n",
    "                new_params.append((name + '.weight', normal_init(module.weight))) # Initialize the weights of the embedding layer\n",
    "        self = self.update(utils.tree_unflatten(new_params)) # Update the model with the new parameters\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def __call__(self, x):\n",
    "        B, T = x.shape # (B = batch_size, T = ctx_len). x is the input tensor\n",
    "        # print(f\"input tensor: \\n {x} \\n\")\n",
    "        # print(f\"x.shape: \\n {x.shape} \\n\")\n",
    "        tok_emb = self.wte(x) # (B, T, n_emb) - Get the embeddings of the tokens\n",
    "        # print(f\"token embedding: \\n {tok_emb} \\n\")\n",
    "        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb) - Get the embeddings of the positions.  arange(T) creates a tensor with values from 0 to T-1 because T is the length of the context and minus 1 because the index starts from 0.\n",
    "        # how it works is that the first position will have the first embedding, the second position will have the second embedding, and so on.\n",
    "        # print(f\"position embedding: \\n {pos_emb} \\n\")\n",
    "        x = tok_emb + pos_emb # (B, T, n_emb) - Add the token and position embeddings\n",
    "        # print(f\"token + position embedding: \\n {x} \\n\")\n",
    "        x = self.blocks(x) # (B, T, n_emb) - Pass the embeddings through the transformer blocks\n",
    "        x = self.ln_f(x) # (B, T, b_emb) - Apply the final layer norm\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size) - Get the logits for the next token prediction\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT()\n",
    "mx.eval(model.parameters()) # Create the model params (mlx is lazy evaluation)\n",
    "loss_and_grad = nn.value_and_grad(model, loss_fn)\n",
    "lr = 0.1\n",
    "optimizer = optim.AdamW(learning_rate=lr) # We use the AdamW optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase\n",
    "\n",
    "1. **Epoch Iteration**: \n",
    "   - The loop begins by iterating over a predefined number of epochs (`num_epochs`).\n",
    "   - At the start of each epoch, the model is set to training mode using `model.train(True)`.\n",
    "\n",
    "2. **Batch Processing**:\n",
    "   - The training data is divided into batches using the `get_batches` function, which yields pairs of input data and corresponding labels.\n",
    "   - For each batch:\n",
    "     - The loss and gradients are computed by passing the input and label through the model using the `loss_and_grad` function.\n",
    "     - The optimizer updates the model parameters based on the computed gradients.\n",
    "     - The running loss, which accumulates the total loss over the batches, is updated by adding the loss of the current batch.\n",
    "\n",
    "3. **Parameter and Optimizer State Evaluation**:\n",
    "   - After processing each batch, the model parameters and optimizer state are evaluated using `mx.eval`. This step ensures that the model parameters are synchronized correctly.\n",
    "\n",
    "4. **Average Training Loss**:\n",
    "   - At the end of the training phase for the epoch, the average training loss is calculated by dividing the accumulated running loss by the number of batches (`batch_cnt`).\n",
    "\n",
    "### Evaluation Phase\n",
    "\n",
    "1. **Set Evaluation Mode**:\n",
    "   - After the training phase, the model is set to evaluation mode using `model.train(False)`. This disables dropout and other training-specific layers.\n",
    "\n",
    "2. **Validation Batch Processing**:\n",
    "   - The validation data is also divided into batches using the `get_batches` function.\n",
    "   - For each validation batch:\n",
    "     - The loss is computed using the `loss_fn` function, which does not calculate gradients (since we are only evaluating the model).\n",
    "     - The running loss accumulates the total loss over the validation batches.\n",
    "\n",
    "3. **Average Validation Loss**:\n",
    "   - At the end of the validation phase for the epoch, the average validation loss is calculated similarly to the training loss.\n",
    "\n",
    "### Logging\n",
    "\n",
    "- **Epoch Summary**:\n",
    "  - At the end of each epoch, the average training and validation losses are printed out. This helps in monitoring the model's performance and identifying potential overfitting or underfitting.\n",
    "\n",
    "\n",
    "This training loop effectively combines gradient descent optimization with batch processing and periodic evaluation to train a model over multiple epochs. The use of running averages and epoch-level summaries helps in assessing the model's learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[441], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m batch_cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m loss, grads \u001b[38;5;241m=\u001b[39m loss_and_grad(model, \u001b[38;5;28minput\u001b[39m, label)\n\u001b[0;32m---> 10\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# compute new parameters and optimizer state\u001b[39;00m\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/optimizers/optimizers.py:29\u001b[0m, in \u001b[0;36mOptimizer.update\u001b[0;34m(self, model, gradients)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlx.nn.Module\u001b[39m\u001b[38;5;124m\"\u001b[39m, gradients: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the gradients to the parameters of the model and update the\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    model with the new parameters.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m                          via :func:`mlx.nn.value_and_grad`.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     model\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/optimizers/optimizers.py:88\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, gradients, parameters)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Apply the update\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:51\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:52\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 52\u001b[0m         k: \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     54\u001b[0m     }\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:51\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:52\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 52\u001b[0m         k: \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     54\u001b[0m     }\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:46\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     45\u001b[0m     TreeType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(tree)\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     52\u001b[0m         k: tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[k] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     54\u001b[0m     }\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:47\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     45\u001b[0m     TreeType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(tree)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[0;32m---> 47\u001b[0m         \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     52\u001b[0m         k: tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[k] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     54\u001b[0m     }\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:51\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:52\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 52\u001b[0m         k: \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     54\u001b[0m     }\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:51\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:52\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 52\u001b[0m         k: \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     54\u001b[0m     }\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:51\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:52\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeType(\n\u001b[1;32m     47\u001b[0m         tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[i] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 52\u001b[0m         k: \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     54\u001b[0m     }\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(tree, \u001b[38;5;241m*\u001b[39mrest)\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/utils.py:56\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     52\u001b[0m         k: tree_map(fn, child, \u001b[38;5;241m*\u001b[39m(r[k] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest), is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     54\u001b[0m     }\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/optimizers/optimizers.py:471\u001b[0m, in \u001b[0;36mAdamW.apply_single\u001b[0;34m(self, gradient, parameter, state)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps)\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_decay \u001b[38;5;241m=\u001b[39m weight_decay\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, gradient: mx\u001b[38;5;241m.\u001b[39marray, parameter: mx\u001b[38;5;241m.\u001b[39marray, state: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    472\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs the AdamW parameter update by modifying the parameters\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    passed into Adam.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\u001b[38;5;241m.\u001b[39mastype(gradient\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train(True)\n",
    "    running_loss = 0\n",
    "    batch_cnt = 0\n",
    "    for input, label in get_batches(X_train, y_train, batch_size):\n",
    "        batch_cnt += 1\n",
    "        loss, grads = loss_and_grad(model, input, label)\n",
    "        optimizer.update(model, grads)\n",
    "        running_loss += loss.item()\n",
    "        # compute new parameters and optimizer state\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "    avg_train_loss = running_loss / batch_cnt\n",
    "    model.train(False) # set eval mode\n",
    "    running_loss = 0\n",
    "    batch_cnt = 0\n",
    "    for input, label in get_batches(X_val, y_val, batch_size):\n",
    "        batch_cnt += 1\n",
    "        loss = loss_fn(model, input, label)\n",
    "        running_loss += loss.item()\n",
    "    avg_val_loss = running_loss / batch_cnt\n",
    "    print(f\"Epoch {epoch:2} | train = {avg_train_loss:.4f} | val = {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = decode(model.generate(1000)[0].tolist())\n",
    "print(completion)\n",
    "with open('completions.txt', 'w') as f:\n",
    "    f.write(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
