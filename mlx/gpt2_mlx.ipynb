{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Install mlx and run the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import mlx.utils as utils\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to training an LLM is collecting a large corpus of text data and then tokenizing it. Tokenization is the process of mapping text to integers, which can be fed into the LLM. Our training corpus for this model will be the works of Shakespeare concatenated into one file. This is roughly 1 million characters and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "with open('../input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the file as a single long string into the text variable. Then we use the set() function to get all the unique characters in the text which will be our vocabulary. By printing vocab you can see all the characters in our vocabulary as one string, and we have a total of 65 characters which till be our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(text)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then wrap a list around it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G',\n",
       " '\\n',\n",
       " 'B',\n",
       " 'v',\n",
       " 'L',\n",
       " \"'\",\n",
       " ';',\n",
       " '!',\n",
       " 't',\n",
       " 'a',\n",
       " ',',\n",
       " 'Z',\n",
       " 'C',\n",
       " ':',\n",
       " 'W',\n",
       " 'R',\n",
       " '$',\n",
       " 'F',\n",
       " 'x',\n",
       " 'I',\n",
       " 'i',\n",
       " ' ',\n",
       " 'c',\n",
       " 'J',\n",
       " 'Y',\n",
       " 'Q',\n",
       " 'l',\n",
       " 'p',\n",
       " '?',\n",
       " 'O',\n",
       " '&',\n",
       " 'X',\n",
       " 'w',\n",
       " 'g',\n",
       " 'T',\n",
       " 'q',\n",
       " 'd',\n",
       " '-',\n",
       " 'K',\n",
       " 'y',\n",
       " '.',\n",
       " 'D',\n",
       " 'U',\n",
       " 'P',\n",
       " 'N',\n",
       " 'e',\n",
       " 'j',\n",
       " 'z',\n",
       " 'o',\n",
       " 'k',\n",
       " 'M',\n",
       " 'b',\n",
       " 'm',\n",
       " 'E',\n",
       " 'A',\n",
       " '3',\n",
       " 'h',\n",
       " 'r',\n",
       " 'n',\n",
       " 'S',\n",
       " 'H',\n",
       " 'V',\n",
       " 'u',\n",
       " 'f',\n",
       " 's']"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(text))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll sort it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"vocab_size: {vocab_size}\")\n",
    "\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from vocab to integers\n",
    "itos = {i:c for i,c in enumerate(vocab)} # int to string\n",
    "stoi = {c:i for i,c in enumerate(vocab)} # string to int\n",
    "encode = lambda x: [stoi[c] for c in x] # encode string to int\n",
    "decode = lambda x: ''.join([itos[i] for i in x]) # decode int to string\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "# [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
    "print(decode(encode(\"hello world\")))\n",
    "# hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encode(text) # encode the entire text\n",
    "split = int(0.9 * len(data)) # 90% train, 10% validation\n",
    "train_data = data[:split] # first 90%\n",
    "val_data = data[split:] # last 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58]\n"
     ]
    }
   ],
   "source": [
    "ctx_len = 4\n",
    "print(train_data[:ctx_len + 1])\n",
    "# [18, 47, 56, 57, 58,  1, 15, 47, 58]\n",
    "# x: [18, 47, 56, 57, 58,  1, 15, 47] | y: 58\n",
    "\n",
    "# 8 sub examples\n",
    "# [18] --> 47\n",
    "# [18, 47] --> 56\n",
    "# [18, 47, 56] --> 57\n",
    "# [18, 47, 56, 57] --> 58\n",
    "# [18, 47, 56, 57, 58] --> 1\n",
    "# [18, 47, 56, 57, 58, 1] --> 15\n",
    "# [18, 47, 56, 57, 58, 1, 15] --> 47\n",
    "# [18, 47, 56, 57, 58, 1, 15, 47] --> 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  [18, 47, 56, 57]\n",
      "labels:  [47, 56, 57, 58]\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs: \", train_data[:ctx_len])\n",
    "print(\"labels: \", train_data[1:ctx_len+1]) # labels = inputs indexed 1 higher\n",
    "# inputs: [18, 47, 56, 57, 58,  1, 15, 47]\n",
    "# labels: [47, 56, 57, 58,  1, 15, 47, 58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation datasets\n",
    "ctx_len = 4\n",
    "X_train = mx.array([train_data[i:i+ctx_len] for i in range(0, len(train_data) - ctx_len, ctx_len)]) \n",
    "y_train = mx.array([train_data[i+1:i+ctx_len+1] for i in range(0, len(train_data) - ctx_len, ctx_len)]) \n",
    "X_val = mx.array([val_data[i:i+ctx_len] for i in range(0, len(val_data) - ctx_len, ctx_len)])\n",
    "y_val = mx.array([val_data[i+1:i+ctx_len+1] for i in range(0, len(val_data) - ctx_len, ctx_len)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, y, b_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        ix = np.arange(X.shape[0])\n",
    "        np.random.shuffle(ix)\n",
    "        ix = mx.array(ix)\n",
    "        X = X[ix]\n",
    "        y = y[ix]\n",
    "    for i in range(0, X.shape[0], b_size):\n",
    "        input = X[i:i+b_size]\n",
    "        label = y[i:i+b_size]\n",
    "        yield input, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice the output below for the label array, 'y', is shifted to the left by 1 in order to predict the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[43, 57, 58, 1]], dtype=int32)\n",
      "array([[57, 58, 1, 58]], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "batch = get_batches(X_train, y_train, 1)\n",
    "for X, y in batch:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_len = 128\n",
    "n_emb = 128\n",
    "dropout = 0.1\n",
    "head_size = 128\n",
    "n_heads = 4 \n",
    "n_layers = 3 \n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, head_size):\n",
    "#         super().__init__()\n",
    "#         self.head_size = head_size # Define the head size of the attention mechanism \n",
    "#         self.k_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the key projection\n",
    "#         self.q_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the query projection\n",
    "#         self.v_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the value projection\n",
    "#         indices = mx.arange(ctx_len) # Create a tensor with values from 0 to ctx_len - 1\n",
    "#         print(f\"indices: \\n {indices} \\n\")\n",
    "#         mask = indices[:, None] < indices[None] # If the value of the first tensor is less than the value of the second tensor, the value of the mask tensor is True, otherwise False which means that the mask tensor is a lower triangular matrix\n",
    "#         print(f\"mask: \\n {mask} \\n\")\n",
    "#         self._causal_mask = mask * -1e9 # Multiply the mask tensor by -1e9 to get a tensor with -1e9 where the value of the first tensor is less than the value of the second tensor\n",
    "#         print(f\"mask: \\n {self._causal_mask} \\n\")\n",
    "#         self.c_proj = nn.Linear(head_size, n_emb) # output projection layer to get the output of the attention mechanism\n",
    "#         self.resid_dropout = nn.Dropout(dropout) # Define the dropout layer for the residual connection\n",
    "#     def __call__(self, x): # shapes commented\n",
    "#         B, T, C = x.shape # (batch_size, ctx_len, n_emb) - x is the input tensor\n",
    "#         K = self.k_proj(x) # (B, T, head_size) - Project the keys\n",
    "#         Q = self.q_proj(x) # (B, T, head_size) - Project the queries\n",
    "#         V = self.v_proj(x) # (B, T, head_size) - Project the values\n",
    "#         attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size) # We use K.transpose([0, 2, 1]) to transpose the second and third dimensions of K. This is because we want to multiply the queries with the keys. The shape of the attention weights is (B, T, T) \n",
    "#         # attn_weights.shape = (B, T, T)\n",
    "#         attn_weights = attn_weights + self._causal_mask # Add the causal mask to the attention weights\n",
    "#         attn_weights = mx.softmax(attn_weights, axis=-1) # Apply the softmax function to the attention weights to get the attention scores\n",
    "#         o = (attn_weights @ V) # (B, T, head_size) - Multiply the attention scores with the values to get the output\n",
    "#         o = self.c_proj(self.resid_dropout(o)) # (B, T, n_emb) - Apply the output projection layer to the output\n",
    "#         return o # Return the output of the attention mechanism which will be used as the input to the feedforward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size # Define the head size of the attention mechanism \n",
    "        self.k_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the key projection\n",
    "        self.q_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the query projection\n",
    "        self.v_proj = nn.Linear(n_emb, head_size, bias=False) # Linear layer for the value projection\n",
    "        indices = mx.arange(ctx_len) # Create a tensor with values from 0 to ctx_len - 1\n",
    "        # print(f\"indices: \\n {indices} \\n\")\n",
    "        mask = indices[:, None] < indices[None] # If the value of the first tensor is less than the value of the second tensor, the value of the mask tensor is True, otherwise False which means that the mask tensor is a lower triangular matrix\n",
    "        # print(f\"mask: \\n {mask} \\n\")\n",
    "        self._causal_mask = mask * -1e9 # Multiply the mask tensor by -1e9 to get a tensor with -1e9 where the value of the first tensor is less than the value of the second tensor\n",
    "        # print(f\"mask: \\n {self._causal_mask} \\n\")\n",
    "        self.c_proj = nn.Linear(head_size, n_emb) # output projection layer to get the output of the attention mechanism\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout) # Define the dropout layer for the residual connection\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def __call__(self, x): # shapes commented\n",
    "        B, T, C = x.shape # (batch_size, ctx_len, n_emb) - x is the input tensor\n",
    "        K = self.k_proj(x) # (B, T, head_size) - Project the keys\n",
    "        Q = self.q_proj(x) # (B, T, head_size) - Project the queries\n",
    "        V = self.v_proj(x) # (B, T, head_size) - Project the values\n",
    "        mha_shape = (B, T, n_heads, head_size//n_heads) # This is the shape of the multi-head attention mechanism because we want to split the head_size into n_heads\n",
    "        # print(f\"mha_shape: \\n {mha_shape} \\n\")\n",
    "        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # We use mx.as_strided to create a view of the K tensor with the shape (B, n_heads, T, head_size//n_heads) and then transpose the dimensions to get the desired shape\n",
    "        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # We use mx.as_strided to create a view of the Q tensor with the shape (B, n_heads, T, head_size//n_heads) and then transpose the dimensions to get the desired shape\n",
    "        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # We use mx.as_strided to create a view of the V tensor with the shape (B, n_heads, T, head_size//n_heads) and then transpose the dimensions to get the desired shape\n",
    "        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # We use K.transpose([0, 1, 3, 2]) to transpose the second and third dimensions of K. This is because we want to multiply the queries with the keys. The shape of the attention weights is (B, n_heads, T, T)\n",
    "        # print(f\"attn_weights: \\n {attn_weights} \\n\")\n",
    "        attn_weights = attn_weights + self._causal_mask[:T, :T] # Add the causal mask to the attention weights\n",
    "        # print(f\"attn_weights + casual mask: \\n {attn_weights} \\n\")\n",
    "        attn_weights = mx.softmax(attn_weights, axis=-1) # Apply the softmax function to the attention weights to get the attention scores\n",
    "        # print(f\"softmax attn_weights: \\n {attn_weights} \\n\")\n",
    "        attn_weights = self.attn_dropout(attn_weights) # Apply the dropout layer to the attention weights\n",
    "        # print(f\"dropout attn_weights: \\n {attn_weights} \\n\")\n",
    "        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads) - Multiply the attention scores with the values to get the output\n",
    "        # print(f\"output: \\n {o} \\n\")\n",
    "        o = o.transpose([0, 2, 1, 3]).reshape((B, T, head_size)) # We transpose the dimensions of the output and then reshape it to get the desired shape\n",
    "        # print(f\"output reshaped: \\n {o} \\n\")\n",
    "        o = self.c_proj(self.resid_dropout(o)) # (B, T, n_emb) - Apply the output projection layer to the output\n",
    "        # print(f\"output projection: \\n {o} \\n\")\n",
    "        return o # Return the output of the attention mechanism which will be used as the input to the feedforward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple implementation of a feedforward neural network.\n",
    "\n",
    "    Attributes:\n",
    "        c_fc (nn.Linear): Linear layer for the fully connected operation.\n",
    "        gelu (nn.GELU): GELU activation function.\n",
    "        c_proj (nn.Linear): Linear layer for the projection operation.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "\n",
    "    Methods:\n",
    "        __call__(x): Forward pass of the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(n_emb, 4 * n_emb)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        x = self.gelu(self.c_fc(x))\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = MLP() # Feedforward neural network\n",
    "        self.mha = MultiHeadAttention() # Multi-head attention mechanism\n",
    "        print(self.mha)\n",
    "        self.ln_1 = nn.LayerNorm(dims=n_emb) # Layer normalization layer\n",
    "        self.ln_2 = nn.LayerNorm(dims=n_emb) # Layer normalization layer\n",
    "    def __call__(self, x): \n",
    "        x = x + self.mha(self.ln_1(x)) # Add the output of the multi-head attention mechanism to the input tensor\n",
    "        x = x + self.mlp(self.ln_2(x)) # Add the output of the feedforward neural network to the input tensor \n",
    "        return x # Return the output of the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y):\n",
    "    logits = model(x)\n",
    "    B, T, C = logits.shape # (batch_size, seq_len, vocab_size)\n",
    "    logits = logits.reshape(B*T, C)\n",
    "    y = y.reshape(B*T)\n",
    "    loss = nn.losses.cross_entropy(logits, y, reduction='mean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPT(nn.Module): # Define the GPT model\n",
    "    def __init__(self):\n",
    "        super().__init__() # Call the __init__ of the parent class\n",
    "        self.wte = nn.Embedding(vocab_size, n_emb) # Lookup table for embeddings of each token in the vocab (word to embedding) -  n_emb means the size of the embedding vector\n",
    "        self.wpe = nn.Embedding(ctx_len, n_emb) # Lookup table for embeddings of each position in the context (position to embedding)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block() for _ in range(n_layers)],\n",
    "        ) # transformer blocks - n_layers means the number of transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(dims=n_emb) # final layernorm\n",
    "        # print(f\"layernorm: \\n {self.ln_f} \\n\")\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size) # output projection\n",
    "        # print(f\"lm_head: \\n {self.lm_head} \\n\")\n",
    "        self._init_parameters() # Initialize the parameters of the model\n",
    "        # print total number of params on initialization\n",
    "        total_params = sum([p.size for n,p in utils.tree_flatten(self.parameters())]) # Get the total number of parameters\n",
    "        # print(f\"Total params: {(total_params / 1e6):.3f}M\") # Print the total number of parameters in millions\n",
    "\n",
    "    # method of GPT class\n",
    "    def generate(self, max_new_tokens):\n",
    "        ctx = mx.zeros((1, 1), dtype=mx.int32) # (1, 1) - Create a context tensor with zeros\n",
    "        for _ in range(max_new_tokens): # Loop through the number of tokens to generate\n",
    "            logits = self(ctx[:, -ctx_len:]) # pass in last ctx_len characters to get the next token\n",
    "            logits = logits[:, -1, :] # get logits for the next token only\n",
    "            next_tok = mx.random.categorical(logits, num_samples=1) # sample the next token\n",
    "            ctx = mx.concatenate((ctx, next_tok), axis=1) # append the next token to the context\n",
    "        return ctx # return the context\n",
    "    \n",
    "    # method of GPT\n",
    "    def _init_parameters(self):\n",
    "        normal_init = nn.init.normal(mean=0.0, std=0.02) # Initialize the weights with a normal distribution\n",
    "        residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers))) # Initialize the residuals with a normal distribution\n",
    "        new_params = [] # Create a list to store the new parameters\n",
    "        # print(f\"named_modules: \\n {self.named_modules()} \\n\")\n",
    "        for name, module in self.named_modules(): # Loop through the modules of the model\n",
    "            if isinstance(module, nn.layers.linear.Linear): # Check if the module is a linear layer\n",
    "                if 'c_proj' in name: # residual projection layer\n",
    "                    new_params.append((name + '.weight', residual_init(module.weight))) # Initialize the weights of the residual projection layer\n",
    "                else:\n",
    "                    new_params.append((name + '.weight', normal_init(module.weight))) # Initialize the weights of the linear layer\n",
    "                if 'bias' in module: # Check if the module has a bias\n",
    "                    new_params.append((name + '.bias', mx.zeros(module.bias.shape))) # Initialize the bias with zeros\n",
    "            elif isinstance(module, nn.layers.embedding.Embedding): # Check if the module is an embedding layer\n",
    "                new_params.append((name + '.weight', normal_init(module.weight))) # Initialize the weights of the embedding layer\n",
    "        self = self.update(utils.tree_unflatten(new_params)) # Update the model with the new parameters\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def __call__(self, x):\n",
    "        B, T = x.shape # (B = batch_size, T = ctx_len). x is the input tensor\n",
    "        # print(f\"input tensor: \\n {x} \\n\")\n",
    "        # print(f\"x.shape: \\n {x.shape} \\n\")\n",
    "        tok_emb = self.wte(x) # (B, T, n_emb) - Get the embeddings of the tokens\n",
    "        # print(f\"token embedding: \\n {tok_emb} \\n\")\n",
    "        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb) - Get the embeddings of the positions.  arange(T) creates a tensor with values from 0 to T-1 because T is the length of the context and minus 1 because the index starts from 0.\n",
    "        # how it works is that the first position will have the first embedding, the second position will have the second embedding, and so on.\n",
    "        # print(f\"position embedding: \\n {pos_emb} \\n\")\n",
    "        x = tok_emb + pos_emb # (B, T, n_emb) - Add the token and position embeddings\n",
    "        # print(f\"token + position embedding: \\n {x} \\n\")\n",
    "        x = self.blocks(x) # (B, T, n_emb) - Pass the embeddings through the transformer blocks\n",
    "        x = self.ln_f(x) # (B, T, b_emb) - Apply the final layer norm\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size) - Get the logits for the next token prediction\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (k_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "  (q_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "  (v_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "  (c_proj): Linear(input_dims=128, output_dims=128, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.09999999999999998)\n",
      "  (resid_dropout): Dropout(p=0.09999999999999998)\n",
      ")\n",
      "MultiHeadAttention(\n",
      "  (k_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "  (q_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "  (v_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "  (c_proj): Linear(input_dims=128, output_dims=128, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.09999999999999998)\n",
      "  (resid_dropout): Dropout(p=0.09999999999999998)\n",
      ")\n",
      "MultiHeadAttention(\n",
      "  (k_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "  (q_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "  (v_proj): Linear(input_dims=128, output_dims=128, bias=False)\n",
      "  (c_proj): Linear(input_dims=128, output_dims=128, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.09999999999999998)\n",
      "  (resid_dropout): Dropout(p=0.09999999999999998)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GPT()\n",
    "mx.eval(model.parameters()) # Create the model params (mlx is lazy evaluation)\n",
    "loss_and_grad = nn.value_and_grad(model, loss_fn)\n",
    "lr = 0.1\n",
    "optimizer = optim.AdamW(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[374], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# compute new parameters and optimizer state\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m batch_cnt\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# set eval mode\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs=1\n",
    "batch_size=1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train(True)\n",
    "    running_loss = 0\n",
    "    batch_cnt = 0\n",
    "    for input, label in get_batches(X_train, y_train, batch_size):\n",
    "        batch_cnt += 1\n",
    "        loss, grads = loss_and_grad(model, input, label)\n",
    "        optimizer.update(model, grads)\n",
    "        running_loss += loss.item()\n",
    "        # compute new parameters and optimizer state\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "    avg_train_loss = running_loss / batch_cnt\n",
    "    model.train(False) # set eval mode\n",
    "    running_loss = 0\n",
    "    batch_cnt = 0\n",
    "    for input, label in get_batches(X_val, y_val, batch_size):\n",
    "        batch_cnt += 1\n",
    "        loss = loss_fn(model, input, label)\n",
    "        running_loss += loss.item()\n",
    "    avg_val_loss = running_loss / batch_cnt\n",
    "    print(f\"Epoch {epoch:2} | loss = {loss.item():.f4}|train = {avg_train_loss:.4f} | val = {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = decode(model.generate(1000)[0].tolist())\n",
    "print(completion)\n",
    "with open('completions.txt', 'w') as f:\n",
    "    f.write(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices: \n",
      " array([0, 1, 2, 3], dtype=int32) \n",
      "\n",
      "mask: \n",
      " array([[False, True, True, True],\n",
      "       [False, False, True, True],\n",
      "       [False, False, False, True],\n",
      "       [False, False, False, False]], dtype=bool) \n",
      "\n",
      "mask: \n",
      " array([[-0, -1e+09, -1e+09, -1e+09],\n",
      "       [-0, -0, -1e+09, -1e+09],\n",
      "       [-0, -0, -0, -1e+09],\n",
      "       [-0, -0, -0, -0]], dtype=float32) \n",
      "\n",
      "indices: \n",
      " array([0, 1, 2, 3], dtype=int32) \n",
      "\n",
      "mask: \n",
      " array([[False, True, True, True],\n",
      "       [False, False, True, True],\n",
      "       [False, False, False, True],\n",
      "       [False, False, False, False]], dtype=bool) \n",
      "\n",
      "mask: \n",
      " array([[-0, -1e+09, -1e+09, -1e+09],\n",
      "       [-0, -0, -1e+09, -1e+09],\n",
      "       [-0, -0, -0, -1e+09],\n",
      "       [-0, -0, -0, -0]], dtype=float32) \n",
      "\n",
      "indices: \n",
      " array([0, 1, 2, 3], dtype=int32) \n",
      "\n",
      "mask: \n",
      " array([[False, True, True, True],\n",
      "       [False, False, True, True],\n",
      "       [False, False, False, True],\n",
      "       [False, False, False, False]], dtype=bool) \n",
      "\n",
      "mask: \n",
      " array([[-0, -1e+09, -1e+09, -1e+09],\n",
      "       [-0, -0, -1e+09, -1e+09],\n",
      "       [-0, -0, -0, -1e+09],\n",
      "       [-0, -0, -0, -0]], dtype=float32) \n",
      "\n",
      "named_modules: \n",
      " [('', GPT(\n",
      "  (wte): Embedding(65, 6)\n",
      "  (wpe): Embedding(4, 6)\n",
      "  (blocks): Sequential(\n",
      "    (layers.0): Block(\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "        (gelu): GELU()\n",
      "        (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "        (dropout): Dropout(p=0.09999999999999998)\n",
      "      )\n",
      "      (mha): MultiHeadAttention(\n",
      "        (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "        (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "        (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "        (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "        (resid_dropout): Dropout(p=0.09999999999999998)\n",
      "      )\n",
      "      (ln_1): LayerNorm(6, eps=1e-05, affine=True)\n",
      "      (ln_2): LayerNorm(6, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (layers.1): Block(\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "        (gelu): GELU()\n",
      "        (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "        (dropout): Dropout(p=0.09999999999999998)\n",
      "      )\n",
      "      (mha): MultiHeadAttention(\n",
      "        (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "        (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "        (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "        (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "        (resid_dropout): Dropout(p=0.09999999999999998)\n",
      "      )\n",
      "      (ln_1): LayerNorm(6, eps=1e-05, affine=True)\n",
      "      (ln_2): LayerNorm(6, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (layers.2): Block(\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "        (gelu): GELU()\n",
      "        (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "        (dropout): Dropout(p=0.09999999999999998)\n",
      "      )\n",
      "      (mha): MultiHeadAttention(\n",
      "        (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "        (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "        (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "        (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "        (resid_dropout): Dropout(p=0.09999999999999998)\n",
      "      )\n",
      "      (ln_1): LayerNorm(6, eps=1e-05, affine=True)\n",
      "      (ln_2): LayerNorm(6, eps=1e-05, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm(6, eps=1e-05, affine=True)\n",
      "  (lm_head): Linear(input_dims=6, output_dims=65, bias=True)\n",
      ")), ('lm_head', Linear(input_dims=6, output_dims=65, bias=True)), ('ln_f', LayerNorm(6, eps=1e-05, affine=True)), ('blocks', Sequential(\n",
      "  (layers.0): Block(\n",
      "    (mlp): MLP(\n",
      "      (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "      (gelu): GELU()\n",
      "      (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "      (dropout): Dropout(p=0.09999999999999998)\n",
      "    )\n",
      "    (mha): MultiHeadAttention(\n",
      "      (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "      (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "      (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "      (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "      (resid_dropout): Dropout(p=0.09999999999999998)\n",
      "    )\n",
      "    (ln_1): LayerNorm(6, eps=1e-05, affine=True)\n",
      "    (ln_2): LayerNorm(6, eps=1e-05, affine=True)\n",
      "  )\n",
      "  (layers.1): Block(\n",
      "    (mlp): MLP(\n",
      "      (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "      (gelu): GELU()\n",
      "      (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "      (dropout): Dropout(p=0.09999999999999998)\n",
      "    )\n",
      "    (mha): MultiHeadAttention(\n",
      "      (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "      (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "      (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "      (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "      (resid_dropout): Dropout(p=0.09999999999999998)\n",
      "    )\n",
      "    (ln_1): LayerNorm(6, eps=1e-05, affine=True)\n",
      "    (ln_2): LayerNorm(6, eps=1e-05, affine=True)\n",
      "  )\n",
      "  (layers.2): Block(\n",
      "    (mlp): MLP(\n",
      "      (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "      (gelu): GELU()\n",
      "      (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "      (dropout): Dropout(p=0.09999999999999998)\n",
      "    )\n",
      "    (mha): MultiHeadAttention(\n",
      "      (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "      (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "      (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "      (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "      (resid_dropout): Dropout(p=0.09999999999999998)\n",
      "    )\n",
      "    (ln_1): LayerNorm(6, eps=1e-05, affine=True)\n",
      "    (ln_2): LayerNorm(6, eps=1e-05, affine=True)\n",
      "  )\n",
      ")), ('blocks.layers.2', Block(\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "    (gelu): GELU()\n",
      "    (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "    (dropout): Dropout(p=0.09999999999999998)\n",
      "  )\n",
      "  (mha): MultiHeadAttention(\n",
      "    (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "    (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "    (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "    (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "    (resid_dropout): Dropout(p=0.09999999999999998)\n",
      "  )\n",
      "  (ln_1): LayerNorm(6, eps=1e-05, affine=True)\n",
      "  (ln_2): LayerNorm(6, eps=1e-05, affine=True)\n",
      ")), ('blocks.layers.2.ln_2', LayerNorm(6, eps=1e-05, affine=True)), ('blocks.layers.2.ln_1', LayerNorm(6, eps=1e-05, affine=True)), ('blocks.layers.2.mha', MultiHeadAttention(\n",
      "  (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "  (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "  (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "  (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "  (resid_dropout): Dropout(p=0.09999999999999998)\n",
      ")), ('blocks.layers.2.mha.resid_dropout', Dropout(p=0.09999999999999998)), ('blocks.layers.2.mha.c_proj', Linear(input_dims=2, output_dims=6, bias=True)), ('blocks.layers.2.mha.v_proj', Linear(input_dims=6, output_dims=2, bias=False)), ('blocks.layers.2.mha.q_proj', Linear(input_dims=6, output_dims=2, bias=False)), ('blocks.layers.2.mha.k_proj', Linear(input_dims=6, output_dims=2, bias=False)), ('blocks.layers.2.mlp', MLP(\n",
      "  (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "  (gelu): GELU()\n",
      "  (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "  (dropout): Dropout(p=0.09999999999999998)\n",
      ")), ('blocks.layers.2.mlp.dropout', Dropout(p=0.09999999999999998)), ('blocks.layers.2.mlp.c_proj', Linear(input_dims=24, output_dims=6, bias=True)), ('blocks.layers.2.mlp.gelu', GELU()), ('blocks.layers.2.mlp.c_fc', Linear(input_dims=6, output_dims=24, bias=True)), ('blocks.layers.1', Block(\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "    (gelu): GELU()\n",
      "    (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "    (dropout): Dropout(p=0.09999999999999998)\n",
      "  )\n",
      "  (mha): MultiHeadAttention(\n",
      "    (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "    (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "    (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "    (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "    (resid_dropout): Dropout(p=0.09999999999999998)\n",
      "  )\n",
      "  (ln_1): LayerNorm(6, eps=1e-05, affine=True)\n",
      "  (ln_2): LayerNorm(6, eps=1e-05, affine=True)\n",
      ")), ('blocks.layers.1.ln_2', LayerNorm(6, eps=1e-05, affine=True)), ('blocks.layers.1.ln_1', LayerNorm(6, eps=1e-05, affine=True)), ('blocks.layers.1.mha', MultiHeadAttention(\n",
      "  (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "  (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "  (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "  (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "  (resid_dropout): Dropout(p=0.09999999999999998)\n",
      ")), ('blocks.layers.1.mha.resid_dropout', Dropout(p=0.09999999999999998)), ('blocks.layers.1.mha.c_proj', Linear(input_dims=2, output_dims=6, bias=True)), ('blocks.layers.1.mha.v_proj', Linear(input_dims=6, output_dims=2, bias=False)), ('blocks.layers.1.mha.q_proj', Linear(input_dims=6, output_dims=2, bias=False)), ('blocks.layers.1.mha.k_proj', Linear(input_dims=6, output_dims=2, bias=False)), ('blocks.layers.1.mlp', MLP(\n",
      "  (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "  (gelu): GELU()\n",
      "  (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "  (dropout): Dropout(p=0.09999999999999998)\n",
      ")), ('blocks.layers.1.mlp.dropout', Dropout(p=0.09999999999999998)), ('blocks.layers.1.mlp.c_proj', Linear(input_dims=24, output_dims=6, bias=True)), ('blocks.layers.1.mlp.gelu', GELU()), ('blocks.layers.1.mlp.c_fc', Linear(input_dims=6, output_dims=24, bias=True)), ('blocks.layers.0', Block(\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "    (gelu): GELU()\n",
      "    (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "    (dropout): Dropout(p=0.09999999999999998)\n",
      "  )\n",
      "  (mha): MultiHeadAttention(\n",
      "    (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "    (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "    (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "    (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "    (resid_dropout): Dropout(p=0.09999999999999998)\n",
      "  )\n",
      "  (ln_1): LayerNorm(6, eps=1e-05, affine=True)\n",
      "  (ln_2): LayerNorm(6, eps=1e-05, affine=True)\n",
      ")), ('blocks.layers.0.ln_2', LayerNorm(6, eps=1e-05, affine=True)), ('blocks.layers.0.ln_1', LayerNorm(6, eps=1e-05, affine=True)), ('blocks.layers.0.mha', MultiHeadAttention(\n",
      "  (k_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "  (q_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "  (v_proj): Linear(input_dims=6, output_dims=2, bias=False)\n",
      "  (c_proj): Linear(input_dims=2, output_dims=6, bias=True)\n",
      "  (resid_dropout): Dropout(p=0.09999999999999998)\n",
      ")), ('blocks.layers.0.mha.resid_dropout', Dropout(p=0.09999999999999998)), ('blocks.layers.0.mha.c_proj', Linear(input_dims=2, output_dims=6, bias=True)), ('blocks.layers.0.mha.v_proj', Linear(input_dims=6, output_dims=2, bias=False)), ('blocks.layers.0.mha.q_proj', Linear(input_dims=6, output_dims=2, bias=False)), ('blocks.layers.0.mha.k_proj', Linear(input_dims=6, output_dims=2, bias=False)), ('blocks.layers.0.mlp', MLP(\n",
      "  (c_fc): Linear(input_dims=6, output_dims=24, bias=True)\n",
      "  (gelu): GELU()\n",
      "  (c_proj): Linear(input_dims=24, output_dims=6, bias=True)\n",
      "  (dropout): Dropout(p=0.09999999999999998)\n",
      ")), ('blocks.layers.0.mlp.dropout', Dropout(p=0.09999999999999998)), ('blocks.layers.0.mlp.c_proj', Linear(input_dims=24, output_dims=6, bias=True)), ('blocks.layers.0.mlp.gelu', GELU()), ('blocks.layers.0.mlp.c_fc', Linear(input_dims=6, output_dims=24, bias=True)), ('wpe', Embedding(4, 6)), ('wte', Embedding(65, 6))] \n",
      "\n",
      "Total params: 0.002M\n",
      "token embedding shape: (65, 6)\n",
      "positional embedding shape: (4, 6)\n",
      "\n",
      "input tensor: \n",
      " array([[58, 46, 1, 63]], dtype=int32) \n",
      "\n",
      "x.shape: \n",
      " (1, 4) \n",
      "\n",
      "token embedding: \n",
      " array([[[-0.00377761, -0.00179952, -0.0405101, 0.0180518, -0.0057386, -0.0142194],\n",
      "        [-0.0304012, 0.00573724, 0.0191489, 0.0252225, 0.032293, 0.00570248],\n",
      "        [0.0430432, -0.0496199, 0.00242303, 0.0253798, -0.0314015, -0.0264305],\n",
      "        [-0.0272118, 0.00740625, 0.0524058, -0.0400309, 0.0316882, 0.00390468]]], dtype=float32) \n",
      "\n",
      "position embedding: \n",
      " array([[0.00730351, 0.00772071, -0.00524605, -0.000217068, -0.0143058, 0.00816989],\n",
      "       [-0.0257161, -0.00969443, -0.0149627, 0.00820811, 0.0184021, -0.00465734],\n",
      "       [-0.0140819, -0.0285076, 0.0149682, -0.0229162, 0.0137229, -0.0174791],\n",
      "       [-0.0155423, 0.0140691, 0.0343618, 0.0186416, 0.0189845, -0.00488297]], dtype=float32) \n",
      "\n",
      "token + position embedding: \n",
      " array([[[0.0035259, 0.00592119, -0.0457561, 0.0178348, -0.0200444, -0.00604955],\n",
      "        [-0.0561173, -0.00395719, 0.00418612, 0.0334306, 0.0506951, 0.00104514],\n",
      "        [0.0289613, -0.0781275, 0.0173912, 0.00246356, -0.0176785, -0.0439096],\n",
      "        [-0.0427541, 0.0214753, 0.0867675, -0.0213893, 0.0506727, -0.000978293]]], dtype=float32) \n",
      "\n",
      "mha_shape: \n",
      " (1, 4, 4, 0) \n",
      "\n",
      "attn_weights: \n",
      " array([[[[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]],\n",
      "        [[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]],\n",
      "        [[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]],\n",
      "        [[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]]]], dtype=float32) \n",
      "\n",
      "attn_weights + casual mask: \n",
      " array([[[[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]],\n",
      "        [[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]],\n",
      "        [[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]],\n",
      "        [[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]]]], dtype=float32) \n",
      "\n",
      "softmax attn_weights: \n",
      " array([[[[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]],\n",
      "        [[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]],\n",
      "        [[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]],\n",
      "        [[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]]]], dtype=float32) \n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiHeadAttention' object has no attribute 'attn_dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[216], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional embedding shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mwpe\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ctx_len x n_emb (8, 6)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# You'll see the token embeddings, positional embeddings, and the sum of the two embeddings for the first batch of the training data.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[214], line 56\u001b[0m, in \u001b[0;36mGPT.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B, T, n_emb) - Add the token and position embeddings\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken + position embedding: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, n_emb) - Pass the embeddings through the transformer blocks\u001b[39;00m\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x) \u001b[38;5;66;03m# (B, T, b_emb) - Apply the final layer norm\u001b[39;00m\n\u001b[1;32m     58\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B, T, vocab_size) - Get the logits for the next token prediction\u001b[39;00m\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/nn/layers/containers.py:23\u001b[0m, in \u001b[0;36mSequential.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 23\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[212], line 9\u001b[0m, in \u001b[0;36mBlock.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Add the output of the multi-head attention mechanism to the input tensor\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)) \u001b[38;5;66;03m# Add the output of the feedforward neural network to the input tensor \u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[210], line 34\u001b[0m, in \u001b[0;36mMultiHeadAttention.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39msoftmax(attn_weights, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Apply the softmax function to the attention weights to get the attention scores\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax attn_weights: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_dropout\u001b[49m(attn_weights) \u001b[38;5;66;03m# Apply the dropout layer to the attention weights\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout attn_weights: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m o \u001b[38;5;241m=\u001b[39m (attn_weights \u001b[38;5;241m@\u001b[39m V) \u001b[38;5;66;03m# (B, n_heads, T, head_size//n_heads) - Multiply the attention scores with the values to get the output\u001b[39;00m\n",
      "File \u001b[0;32m~/nn/.venv/lib/python3.11/site-packages/mlx/nn/layers/base.py:137\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mModule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiHeadAttention' object has no attribute 'attn_dropout'"
     ]
    }
   ],
   "source": [
    "# model = GPT()\n",
    "model = GPT()\n",
    "\n",
    "print(f\"token embedding shape: {model.wte.weight.shape}\")\n",
    "# vocab_size x n_emb (65, 6)\n",
    "\n",
    "print(f\"positional embedding shape: {model.wpe.weight.shape}\\n\")\n",
    "# ctx_len x n_emb (8, 6)\n",
    "\n",
    "model(X)\n",
    "# You'll see the token embeddings, positional embeddings, and the sum of the two embeddings for the first batch of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
